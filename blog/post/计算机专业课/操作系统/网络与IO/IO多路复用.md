基本概念：对一个socket监听，accept可以得到连接socket对象
# TLDR：
epoll全流程：
- 服务启动，主线程监听特定socket
- 主线程获取若干连接socket，将其送入内核中的红黑树中，并给好回调函数
- 事件发生，回调函数将其写入内核中的就绪链表。
- 主线程使用epoll_wait, 传给内核一个数组地址，内核将fd写入该数组，并返回数量
- 主线程对fd进行操作（这个fd是一个event对象，里面是事件相关信息和fd信息）
# 基本的socket模型：主线程监听一个socket，将获取的socket直接交给处理程序
一个tcp socket即IP+端口， 而用于io的是一个socket对象
- 对一个socket进行监听后， accept获取的是一个socket对象
- 这是两个概念，但是同名了。
通讯时， server创建一个socket， 绑定ip和端口，然后开始监听。调用accept阻塞等待连接。
当client请求时， 就开始三次握手，这个过程中， server为每个socket维护两个连接队列：
- syn_rcvd： 半连接队列， 没有完成三次握手
- established：全连接队列， 已经完成握手的连接
两个队列中存储的是五元组， 用以代表连接：
- 源IP
- 源端口
- 目标IP
- 目标端口
- 协议号
只要全连接不是空的， server就会从中拿一个已经连接上的给应用程序， 进行io
这个socket会以文件描述符的形式被分配给进程[[文件描述符 FD]]
## 短连接和长连接中涉及的系统调用
### 短连接
server：
- socket->bind->listen->recv->send->close
client:
- socket->connect->send->recv
### 长连接， 如ws， http/1.1， 数据库连接等
server：
- socket->bind->listen->accept()
	- 然后循环进行：
		- recv： 如果没数据会一直阻塞在这
		- send
	- close: 超时或者主动断开
## 不同“文件”的系统调用
- 网络：socket
- 文件：read/write
- 标准输入/输出：stdin/stdout
- 管道：pipe
- 设备文件: /dev/xxx
## recv和read的区别
recv是read的拓展：
- read是一个通用系统调用
- recv加上了控制功能，用于特化socket操作，send/write也一样
# 高并发IO的可能
基本的socket模型只能每个线程对一个socket对象进行io， 性能很差。
先从两个角度出发：
- fd数量： 单进程默认只能打开1024个fd， 这个和实际网络连接数差太多
- 系统内存：每个连接都会在系统中占据一定内存
对于经典C10K问题，即1w个请求，上述资源问题还是可以满足的， 但是单线程io速度会变成瓶颈

### 多进程
父进程关注监听，每次accpet到一个socket， 就fork一个子进程，这个子进程共享fd， 可以打开这个socket，此时他去io就行了。 主要缺点：
- 资源调度麻烦，消耗也多
- 上下文切换非常耗时
这不太可能实现C10K， 1w个进程资源消耗太多了
### 多线程
使用线程池维护多个io线程， accpet后将socket交给一个线程去处理。
- 这个分配socket的过程一般用生产者-消费者模式实现
	- 不能阻塞等线程，得用队列
- 此时所有io队列都会去队列中竞争， 需要加锁
也难以实现C10K， 1w个线程也调度不过来。

# 监听/连接套接字的生命周期与区别
- server： 创建监听套接字， 由主线程或者IO复用线程创建并管理
	- 需要操作为，socket， bind，listen
- server：监听，接受连接
	- 使用accept，返回连接套接字
- 数据传输阶段
	- server可以对socket进行（recv/read）（send/write）操作

# IO多路复用的思路：一个线程监听socket，将所有连接socket对象统一处理
- 即所有连接socket使用bitmap，链表，红黑树处理
每个连接对应一个线程/进程，就算使用池子， 也涉及了太多的调度/切换问题，是不能接受的。
想要实现C10K， 必须保证，处理连接这个过程非常快速，然后进入io阶段，避免调度问题。

io多路复用， 就是：
- 基于select/poll/epoll这些内核提供的多路复用系统调用
- 使用系统调用从内核获取事件信息
- 其流程为：
	- 在获取事件时， 将所有fd传给内核，让内核返回fd上有事件的连接
	- 用户态对这些连接进行处理
## select
将所有socket都放到一个fd的set中， 然后调用select， 将这个set拷贝到内核中，让内核检查是否有事件，就是遍历这个set，检查到有事件后， 将socket标记为读/写状态，然后再把整个set拷贝回用户态，然后应用这边再遍历一边拿出需要操作的socket。
可见：
- 此时不需要线程去等待socket了， 只要每次提出所有socket去处理就行。
- 但是select需要扫描两次set， 还需要传递两次整个set
select使用固定长度的bitsMap表示集合， 上限也是1024， 可以设置
## poll
不用bitsmap了， 用动态数组， 用链表组织，支持的fd更多了， 但是没有本质区别。
## epoll： 足以实现C10K
主要方法有：
- epoll_create: 创建epfd对象（最后记得关闭epoll对象）
- epoll_ctl： 将需要监视的socket添加到epfd
- epoll_wait： 调用以等待数据
核心思路
- 在内核中使用一个**红黑树**跟踪进程中所有待检测的文件描述符，即用ctl将socket对应的fd加进去
	- 相比与select/poll， 每次都是直接传整个set，使用红黑树维护更方便
		- 需要高频率增删改， 所以用红黑树，不能用数组这种
	- 大大减少了内核和用户态数据拷贝和内存分配
- 使用事件驱动，再维护一个**就绪链表**记录所有就绪事件（等待读写）
	- 当事件发生， 使用提前设定的**回调函数**将其加入这个链表中
	- 当用户调用epoll_wait，只返回有事件发生的fd个数
		- 而不是直接把整个集合传回去
		- 此时不需要查树，而是直接查这个链表，返回里面的元素
			- 其实是写到epoll_events数组中
### 实际使用： epoll如何从内核获取事件
首先在用户态创建一个epoll_events结构的数组，然后在调用epoll_wait的时候， 将这个数组的指针传进去， 内核态会创建events对象并写到对应位置。
- 这个过程是数据从内核态拷贝到用户态内存的过程
- 用户指定这个数据长度， 就可以实现最大连接数控制。
- wait的返回值是事件个数
- event对象主要包括：
	- 事件类型
	- epoll_data，主要是fd号和位置
### epoll事件触发方式
- 边缘触发
	- 当socket上有可读事件发生时， 这个socket只会醒一次， 就算没有调用， 故要保证一次性读完
	- 边缘触发只在wait中参与一次，并发度更高， 但是要配合非阻塞IO，如NIO使用
		- 因为需要循环持续读（不知道有多少数据），如果IO阻塞了，线程也阻塞了
		- 会一直读到read/write报错
	- epoll默认边缘触发
- 水平触发
	- 当socket有可读事件， 持续苏醒， 直到内核缓冲区被读完才结束
	- select/poll默认是水平触发
### 实际读写过程
虽然说是事件触发后拿socket去io， 实际上这些数据都是在内核的缓冲区进行的
拿到socket后， 需要去缓冲区read文件

所以就有一个问题， 事件触发后， socket不一定是可读写的， 可能数据还没准备好，此时如果使用了阻塞式IO， 就会导致程序阻塞。

