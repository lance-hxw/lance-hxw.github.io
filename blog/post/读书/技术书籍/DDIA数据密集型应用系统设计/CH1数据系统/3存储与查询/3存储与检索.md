数据库底层应该如何处理查询和存储?
需要从：业务-查询类型-存储格式 进行分析
# 查询类型
查询类型主要分为
- OLTP（事务）：频繁请求，每次很小
	- 瓶颈：磁盘索引
	- 应用：普遍应用中，如银行交易
	- 如：mysql，一般与数据库交互，支撑业务
- OLAP（分析）：较少请求，每次很大
	- 瓶颈：磁盘带宽
	- 应用：主要是商业分析中
	- 如：clickhouse，一般与数仓交互，用于分析，偶尔用于支撑业务
# log-structured与update-in-place
在oltp中的两类主要存储引擎
log型：只允许追加，任何修改都是追加和文件整体变动
- 基本设计思想是：不要随机写，要顺序写
- 如：levelDB，rocksDB，lucene，cassandra
update-in-place：用页面为单位进行磁盘修改
- 基本设计思想：基于页面进行查询优化
- 如：B树，关系型数据库和部分nosql
# 底层数据结构
- 一般来说，写是很快的，而读是很慢的（朴素是全表查），但读又更常用
此时需要进行读写权衡：
- 用索引改进读性能是正常思路，但是这会让写变慢（每次要改索引）
	- 还费空间
- 此时用日志结构又能增强写性能，但是读取很慢
- 此时可以用树结构增强读取，但是写又慢

在这种权衡框架下，底层数据格式一般设计好，索引则是用户决定如何创建
## 简易索引优化如: Hash索引(bitcask)结合log
使用kv类型的索引,key就是数据原本的key,value是数据条目的起始位置和长度
如果所有索引都能放到内存中,就非常好用
- 写: 文件追加写,更新索引
- 读: 一次内存查询索引,一次磁盘访问(如果没缓存)
这种存储情况是:
- 外存的log
- 内存的hash索引(hash表)
## log型必须要考虑的问题
- 文件格式: 必须紧凑, 如长度+数据
- 删除操作: 写墓碑值,后续compact的时候删掉
- 宕机恢复: 索引是存在内存的,需要重建
	- 一般优化是,每个持久化文件都有自己的索引条目,直接读出来就行
- 写过程中故障: 校验字段,还得区分好数据边界
- 并发控制: 只有一个活动文件,其他文件不可写
	- 但是compact和读是可以并发的
## log型相比原地更新的优点:
- 顺序写而不是随机写, 性能强多个数量级
- 简易并发控制,只需要一直在末尾写,其他文件不可写,所以可以并发的读取和compact等操作
	- 并且不存在新老数据写的原子性问题
- 碎片少,compact是merge的,会消除碎片
## 较为完善的log型存储方案:sst & lsm tree
此前基于hash的简单方案的log是无序的,如果让这个log文件有序,这就是sorted string table
其最大优势是,高效的文件合并,即外部归并
每个持久化的文件都是只读的,并在文件末尾记录最大最小key
- 此时查的时候,只需要在内存保存每个文件的范围
- 然后去文件里二分找就行了

顺序存后,有一个新的trick:
- 由于局部key前缀相同,所以可以再分块,每个块都有共同前缀
- 索引就直接用block作为单位,然后去block中查

### 如何构建sst? 数据原本是乱序进来的
- 内存memtable,用红黑树等结构维护
- 达到threshold后dump到外存
### 如何使用sst?
- 读: 先去memtable查,查不到就去sst文件中从新到旧查
### 如何维护sst?
随着sst越来越多,查找代价会越来越大
就需要进行compact/merge,这个过程中
- 合并小sst为大sst
- gc,删除小sst
- 处理删除情况

### sst最大问题: 宕机内存消失
如何解决? WAL 非常经典

### lsm
上述优化结合起来就是lsm tree,是levelDB的数据存储结构
es和lucene都类似这个

### lsm上的优化

- 优化sst查找: 用bloom filter给每个sst做一些指纹,用于初筛,只需要处理假阳
- 层次化sst: 这样可以控制compact的逻辑
	- 有size-tiered 和leveled(常见)
- rocksDB的优化:
	- 列族
	- 前缀压缩和过滤
	- 键值分离(索引和数据分离),
	- blobDB(大值特殊处理)
## 原地修改方案: B树
同样是高效的点查和范围查
但是是用页来组织的
- 查: 不断二分读页进来, 直到找到叶子节点
- update: 定位到key所在页,修改页后再写回
	- 如果页面空间不够,就要页分裂
	- 涉及写策略(写回,写穿)
此外,一个节点可能对应多个物理page

### B树改进
- wal预写日志,防止写时中断导致结构混乱
- 或者不用wal, 在写的时候用copy on write,如lmdb
- 对中间节点的key压缩,只保留路由信息,这样可以增大分支因子,提升性能
- 给叶子结点做成双链表,即B+
- latch对树结构进行并发控制
## B树和lsm对比
写: lsm更快
读: B树更快
写放大: 需要wal和compact(lsm)处理
写吞吐: B树小,LSM相对大
压缩率: B树碎片多, lsm内存少,且有前缀压缩
高负载情况:
- b树比较稳定
- lsm如果写吞吐上去了,compact跟不上就会有明显写放大,同时compact会占用部分磁盘带宽,数据总量如果大了,compact压力也很大
存储放大: B树有的page不满, lsm有的key要存很多次
并发:
- b树并发控制容易
- lsm一个key有多次出现,需要mvcc控制

# 其他索引结构与实现
## 堆文件
数据是无序存储的，用一个索引维护数据在heap file中的位置，除了能快速查询，也能避免重复插入的荣誉

## cluster与否
即数据按照主键的顺序存储，即数据和索引在一起
非聚簇索引则只存主键
## 覆盖索引
或者叫包含列的索引， 即将索引列放在索引中存储

## 多列索引
此时有两种做法：
- 将多个列编码成一个列
	- 然后按普通索引建立
	- 如B树，只要按元组依次比较就行了
- 使用特殊的数据结构， 如R树
	- mysql中R树用于地理信息
## 全文索引和模糊索引
工程中常用lucene和es进行
其存储结构是类似lsm tree的日志型，但是利用其索引进行模糊匹配的过程，本质上是dfa， 或者说trie

## 全内存结构
此类存储可以分成两类
- 需要持久化
	- 需要WAL，或者定期snapshot，远程备份
	- 如RAMCloud（持久化保证
	- 如redis（弱持久化
- 不持久化
	- 如memcached
内存数据结构除了物理性能外，还能提供一些只能存在于内存的数据抽象， 如queue和set


# OLTP和OLAP
OL意思是在线
TP是事务, AP是分析
在这里:
- 事务: 随机的, 低延迟读写
- 分析: 定期批处理, 高延迟
早期OLAP也用传统数据库, 但是大数据时代转向了数仓,因为传统数据库在AP负载中的吞吐太差.
## 数据仓库
用于AP业务的专用数据存储
使用ETL工具（extract-transform-load，三步）
将数据导入专用数仓，用于后续AP分析
## AP建模
常用有星状维度模型和雪花模型

星状模型：
- 一张事件表
	- 用事件流的方式将数据组织起来，然后用外键指向不同纬度
- 若干维度表
	- 具体数据
雪花模型：类比雪花图案，每个维度都可以二次细分

事件表一般会非常非常宽，可能有数百列
# 列存
列太多了，必须要拆
如何“回表”：
- 一般是用下标对应同一条数据
- 可以用主键来回，但是成本太高，列翻倍
## 列压缩
由于列内数据相似度高，就可以进行压缩

若值域非常小（相比行数），可以直接用bitmap
（一般得小于int位数吧
使用了bitmap后 ，就可以使用RLE游程编码进行压缩
即将29这个数，按照2进制的01，写成数量+值的形式，即29=9个0,1个1,8个0
然后固定首位0，此时就是01循环出现，不用标明，即可写成9,1,8
然后固定总长，则最后一位也不用写，即9,1

bitmap的数据非常适合查询中的逻辑运算

### 列族
是cassandra和hbase中的概念， 来自于bigtable
- 一个列族多个列一同存储，并内嵌row key
- 列不压缩
相当于子表， 还是面向row的存储

### 内存带宽和向量化处理
数据规模太大的瓶颈：
- 内存处理的带宽
- cpu分支预测错误和流水线停顿
内存处理带宽主要是得让存的数据量变少，如使用列压缩等方法

cpu瓶颈主要解决：
- 用列存和压缩让数据尽可能多的缓存在L1中，结合bitmap加速cpu处理
- 使用SIMD，用更少的时钟周期处理更多的数据
	- 数据并行，一般是做成扩展指令集这样
	- 实现自动向量化

## 列存的排序
OLAP的查询往往在使用聚合算子,所以一般存储顺序不重要
如果想排序也可以,此时是对所有行按照一列进行排序后存储,就像lsm一样
注意, 往往不能对多列排序, 此时会破坏数据的局部性,造成性能下降
- 破坏局部性,导致难以压缩
- 查询剪枝效率低下:本来可以按列存最大最小值用于范围查询的,现在不方便
- 多列排序行号映射也会更复杂
在需要多列排序键的时候(clickhouse, snowflake等), 应该在一开始在DDL中声明sort key(a, b), 此时engine会对这两列做物理组织
如果只是查询, 可以用外部排序
### 不同副本可以不同排序
数仓一般会是分布式多副本的, 此时每个副本可以按不同的列排序,然后吧不同的查询请求路由到不同的副本上进行
这个是c-store引入, vertica等数据库实现了这个功能

## 列存的写入
列压缩和排序是读的优化, 因为数仓就是读多写少,且读都是超大规模连续读
这样必然导致数仓中写的困难
B的update in place就不行, 比如在中间写一个record, 就得通知所有的表, 还得让所有行的行号被影响
只能使用log-structure
- 将所有要写的数据在内存中batch好
- 达到阈值后, 批量刷到外存, 和老数据合并
## 列存的聚合: 物化

materialized aggregates, 物化, 或者类似持久化 的聚合
这玩意叫物化视图
其思路为: 将常用的聚合结果直接持久化起来, 本质是:
- 一个缓存
- 一种摘要
这个也是一个 tradeoff, 因为持久化这种东西时:
- 一旦数据被写入 就需要重新生成视图
- 只能在读多写少的场景使用
### 数据立方, data cube
按照不同维度对数据进行聚合得到的视图
