
分片部署在物理节点, 当请求到达, 就需要将其路由到具体的机器上, 此时需要:
- 数据到逻辑分片的映射: 这步一般是基于规则进行的(算法化路由)
- 逻辑分片到物理节点的映射: 这步一般是基于路由表记录的

不过也可以用一致性hash, 直接用hash环而不是具体逻辑分片作为解耦合中间体


这个逻辑相当于服务发现, 类似的, 解决方案有:
- 每个节点都部署全局路由表
	- 客户端随意连接一个节点, 节点返回或者进行路由(迭代或者递归)
	- 优化速度:
		- 加布隆过滤器
	- 路由表很大?
		- 这种路由表肯定是前面算法计算逻辑分片后, 记录逻辑分区到物理节点路由
			- 若是数据到物理分片的路由, 肯定不行
- 用专门的路由转发层进行
	- 此层记录所有分片信息, 然后转发路由
	- 如果这个层崩了怎么办:
		- 集群化部署
		- 元数据高可用存储, 交给外部协调, 只进行路由操作
- 客户端能感知具体节点
	- 客户端里知道路由
		- 这种一般是从配置中心拉取的放在客户端本地进行负载均衡

无论是哪一层感知路由, 都需要分布式协调, 保证路由一致(不会将逻辑节点调度错误或者寻找错误)
- 外部协调器, zk, etcd, 维护轻量路由表, 通过发布订阅, 在路由更新时通知外部节点
- 内部元数据服务器集群, 集群间协调
- 基于协议点对点同步, 如gossip


上述三种分别是在存储节点, 中间件, 客户端感知路由, 但请求最终都要知道路由:
- 客户端->存储节点->路由: 此时客户端要知道存储在哪, 这种一般是用一个轻量网关层随机映射(random负载均衡)
- 客户端->中间件(路由层)->路由: 由于路由层集群不易变动, 所以用dns或者反向代理进行polling就行

# 并行查询执行
一般nosql查询都比较差, 都是基于key或者次级索引上的查询

但是关系型数据库, 特别是大规模并行处理数仓, 查询会比较复杂

此时会分多个stage并在每个stage对每个partition进行并行子查询

一般用于BI, 需要特别优化(见"批处理")