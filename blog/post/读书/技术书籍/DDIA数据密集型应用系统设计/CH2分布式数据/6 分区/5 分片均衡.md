在机器运行过程中:
- 吞吐增加, 需要加机器
- 节点宕机, 需要替换机器
- 数据变大, 需要给机器增配.
所有问题都会导致数据分片在节点之间进行迁移, 这个过程是再平衡(均衡)
均衡之后, 应该满足:
 - 负载在节点间均匀
 - 均衡时要保持可用
 - 减少数据移动, 降低网络和磁盘io压力
# 为什么不能用hash mod N
这样一旦N变化,  几乎所有节点都需要发生均衡操作
因为这种结构, 数据和物理分片是强耦合的, 如何解耦合:
- 数据->逻辑分片, 逻辑分片->物理分片
	- 如一致性hash
	- hash到逻辑分片, 然后调度逻辑分片
# 均衡策略
## 静态分区
逻辑分片数量固定, 并且最好远大于物理分片数.然后维护逻辑分片到物理分片的路由

为什么逻辑分片要足够多:
- 扩容: 增加机器要将一些分片调度上去, 否则有的节点没有分片难以实现扩容逻辑
- 小粒度精细调度
- 解决异构性, 不同物理节点资源差异大, 小粒度逻辑分片可以精细调度

不过不能太多, 静态分区的每个分片也有元数据和调度开销

一般可以取你的节点最高拓展数为初始分区数

静态分片便于维护, 且容易实现. 不过如果数据爆发增长, 就需要重新分区.

## 动态分区
如果分片是基于key range的, 那么数据大概率在定义域上不均匀, 此时静态分区就难以均衡. 所以如果是key range, 就必须要用动态分区.

动态分区的演进:
- 一开始, 数据量少, 只用一个分区
- 若单个分区超过上界, 将其拆成2个
- 若某分区数量过少, 将其与相邻分区合并
	- 如果合并后超过上界, 就分裂
	- 这个阈值一定要设置好, 分裂后两个子分片的size必须离merge_threshold很远, 否则会抖动

不过一开始, 数量很少用一个分片的时候, 并发写入限制大, 所以mongo和hbase支持预分区

key range的动态分区一般对每个分片维护最大最小值即可

hash-based的分片也可以动态分片, 维护slot的最大最小值即可
## 与节点成比例分区
静态动态分区的分区数量都和物理解耦, 这样方便, 但是效率可能不是最好, 还可以:
- 总分区数量和节点数量成正比, 每个节点上的分区数量保持不变
若一个集群上有n个节点,每个节点用有m个分片, 当新节点加入, 就要给他分配m个分片, 就从所有分片(m\*n)上随机找m个分裂后给新节点

这个操作是非常接近一致性hash的画风的, 只要是随机操作, 就能将负载均摊(如果掉节点就是
加负载)

# 自动均衡和手动均衡
要注意:
- 开销: 网络开销, 非常大
- 判断: 要判断是否需要均衡
一般都是半自动+低谷期进行

