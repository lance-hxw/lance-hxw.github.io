共识协议是分布式系统中最直觉而又最复杂的抽象, 可以认为, 共识问题就是分布式系统复杂度的一种表现/集合

Leader选举, 原子提交, 这些在分布式系统中最关键的假设/性质都依赖于共识

要处理共识问题, 需要replication, 事务, 系统模型, 一致性和全序广播等基础知识.

本章会从最基本的共识算法2pc开始, 并介绍更加完善的zab(zk)和raft(etcd, nacos)

# 原子提交和两阶段提交

## 单机提交, 分布式原子提交

单机的原子提交基本依赖数据库层面, 即使用WAL(binlog)+undo log 实现, 事务成功提交的标志是binlog上的提交日志, 如果没有成功提交或者宕机, 就会用undolog 或者直接从binlog识别修复

单机的事务提交是否成功主要看数据写磁盘的顺序是否正确, 即先写数据, 再写提交记录, 事务提交与否取决于提交记录刷盘与否, 这是一个明确的节点, 在这之前会回滚, 在这之后已提交. 

可以看出, 单机的原子性本质上来自磁盘的唯一性

而在分布式场景, 对多节点期望使用单机事务实现分布式事务是不可能的.
一个事务可能在某些节点正常提交, 有的分片节点违反约束要求回滚, 有的节点宕机或者失联没有提交. 
这个时候, 就一定需要一个机制来回滚, 但是如果检查单机事务提交是不行的, 事务提交后是不能撤销的(但是可以用补偿事务实现撤销, 不过这是应用层面实现的保证, 在系统/数据库层面, 这是另一个事务, 与此事务执行无关, 而且这很麻烦)
## 2PC
[[分布式#分布式事务#2PC (prepare-confirm), 确认全部执行成功后, 提交事务]]
2pc会在一些数据库内部使用, 有时也会被当做XA事务供应用层使用

2pc有个关键的变化, 存在一个协调者, Transaction manager
- 实现上, 一般是一个库或者一个单独的服务
- 这个协调者存在单点问题
这个事务提交的节点是协调者收到所有参与者的yes, 做出提交决策
## 基于承诺的系统

单机在发出yes回应后, 自己就不能决定回滚事务了

协调者将提交写入日志后, 整个系统的事务就提交了

- 参与者故障在yes回应后不影响事务提交,  只要协调者最后决定写入commit日志, 然后就会一直通知参与者, 直到参与者收到通知为止

## 协调者故障

协调者故障:
- 发生在prepare之前: 事务直接回滚
- 发生在prepare发出后, commit消息传播完之前:
	- 此时参与者会一致等待, in doubt /uncertain
	- 这个问题是不能通过超时等机制解决的, 因为可能有的节点已经收到commit提交了
	- 只能等TM恢复, 检查他的日志, 这个事务到底提交没, 然后让参与者确定状态
- commit传播完后故障, 此时:
	- 日志已经写入, 系统认为事务已经提交
	- 各个节点已经提交单机事务
## 3PC
[[分布式#分布式事务#3PC]]
试图解决协调者故障带来的系统阻塞, 但在无界停顿下存在非原子问题, 就算用2PC也不用3PC
# 分布式事务的实践

所有分布式事务, 特别是2PC, 都不是很完美, 他们的性能损失太大了, 很多系统都选择不支持分布式事务. 
分布式事务的性能损失来自其算法内生的复杂度: 协调者宕机的额外刷盘操作, 还有协调者和参与者的网络通讯开销, 这个开销还是取决于最慢的那个的

分布式事务可以分成两类:
- 数据库等分布式系统内部使用的, 此类往往优化较好
- 异构分布式事务, 此时多个节点中可能有不同的数据库, 甚至有消息队列等系统, 此时要实现原子提交的复杂度就很高了
## exactly one 消息处理

在异构系统, 或者说任何系统中, 我们首先要保证每个单机都能原子提交, 然后我们只要保证每个信息被有效的处理正好一次, 才能实现分布式事务 
## XA 事务

X/Open XA (eXtended Architecture)协议是异构系统中实现2PC的一个标准

目前大部分传统关系型数据库和消息队列都支持XA

XA是一组和TM交互的C接口, 这个协议也有很多其他语言的版本, 比如java使用JTA

### DTP模型

针对XA定义的分布式事务处理模型:
- AP 应用程序, 定义组成事务的操作, 形成事务边界
- RM 资源管理器, 对应参与者
- TM 事务管理者, 即协调者, 负责协调事务与故障恢复

XA协调者往往作为一个库被加载到AP中, 并追踪所有参与者, 使用本地磁盘日志协调事务

XA驱动会提供回调给协调者, 用于要求参与者准备,提交, 终止

RM通过驱动和AP中的XA协调者实例通讯

## 阻塞持有锁问题

TM故障时, RM会持有锁死等, 导致RM部分数据不可用

如果TM不重启了, 那么RM需要手动恢复

## 改出协调者故障

理论上只要TM恢复就能改出, 但是实践中存在部分孤立未定事务, 比如因为日志故障, 软件bug, 导致TM判断不了事务的最终结果, 此时这些事务就需要在RM上手动处理

重启RM是不行的, 因为2PC要求RM必须一致持有锁, 否则会破坏原子性

手动处理问题是很大的, 因为XA系统故障往往是系统高压,此时手动处理会造成系统的性能大幅降低

XA实现往往会留有后门, 用于暂时违反原子性, 让RM从TM故障改出, 只能用于救急, 且确认失败的事务被破坏不影响系统功能
## 分布式事务的限制

分布式事务的运维复杂度是非常高的

TM本身也是一个数据库系统, 存着事务的决策结果日志等, 所以也需要容错, 可用等性质
- 如果TM是单副本, 那就成了单点. 然而现实中的协调者实现往往就是非HA的, 或者很粗糙
- 很多服务端应用是无状态的, 然后将状态放在外部数据库中, 此时如果AP中有了TM, 他的日志要存本地, 此时就是有状态的了
- XA能力限制, 因为要适配广泛的数据系统, XA只维护了一个最小接口集合, 比如XA不能跨系统检测死锁, 因为这需要提供接口, 获取所有节点的锁情况. XA也不能提供跨系统SSI隔离, 因为要求跨系统冲突检测(SSI相比SI, 需要读写冲突检测)

 - 不过数据库内部的节点间XA就没事, 因为可以单独优化,  但是还是要别的问题, 因为一个RM故障就会导致系统失败, 这是一种故障放大操作(不过这是一致性和HA的权衡)

难道分布式一致性是必须要放弃的么, 不是(CH10 11会解释)

# 用于容错的共识算法 consensus

共识协议的形式化描述:
- 一个节点或者多个节点会propose一些值, 共识协议需要在这些值中做出唯一的decide, 即这个值只能有一个
所以一个共识协议必须满足
- 全局一致性, 任何两个节点的决策都是一致的
- 正直性, 任何节点只能做一次决策, 不会反复变化决定
- 有效性, 一个节点做出决策后, 这个值一定是某个节点的提议, 不是凭空产生的
- 可终止性, 任何非宕机节点最终都能给出决策(收敛)
## 全序广播中的共识算法

最著名的有 VSR, Paxos, Raft, Zab 他们的顶层设计有很多相似之处, 但不完全相同

这些算法是为了投入应用的, 他们都不只是为了对单值做出正直有效可终止的决策, 而是在多个值上进行, 从而变成一种全序广播算法

全序广播就等价于多轮次的共识协议. 如: 假设每个轮次, 用共识协议对全序广播中的一条消息的全局顺序做出决策
- 由于共识协议全局一致, 所以所有节点能用相同的顺序投递消息
- 由于正直性, 所以只能消费一次(只能给出一个id)
- 由于有效性, 消息是不会无效(顺序有效)
- 由于可终止性, 消息不会丢失(不会不知道什么时候执行)

VSR, Raft, Zab都是直接实现了全序广播, 即他们不是每次只对一个达成一致, 而是构建成全序广播系统, 持续对消息顺序达成一致
## 单主 与共识

这里有个很搞的事, 首先考虑单主模型, 单主模型处理所有写入事务并广播到其他节点, 这个广播就是全序的, 这看起来非常简单, 因为复杂度被隐藏了, 即这个主是如何选出来的

如果主总是人工设置, 那么系统复杂度就转移到人工上, 如果系统会自动选主, 那复杂度就转移到选主算法上

那么选主算法是如何进行的呢? 选主算法本质是对leader是谁这个值达成一致, 所以选主需要共识算法.

但是我们的共识算法都是全序广播实现的...

为了讨论有意义, 我们要找一个切入点, 实现一个可行的共识算法
## 纪元编号与法定人数

各种共识算法都需要一个形式上的主节点, 但系统无法保证主节点唯一, 但是可以有一个弱的保证: 定义一个纪元编号(投票编号, 视图编号, 任期编号) 并保证每个任期内, 主节点唯一.

每当主节点被认为下线, 所有认为主节点下线的节点就发起选举, 使用一个更高的纪元编号选出新leader. 这样当系统中出现多个主节点, 纪元编号大的就是胜者

当然"纪元编号大就胜出"不是一个全局view, 主节点并不知道自己有没有被罢免, 所以需要再使用quorum的思路(w个投票成功, r个读取认定自己是主节点)

主节点发出决策时, 必须将其作为proposal发送到每个节点, 并等待法定个数个回复, 发现回复中没有更大的纪元编号, 主节点就可以安心认为自己合法
- 但是这个实现会有变数, 有的放宽的算法可能不需要多数节点

这个过程有点像2pc, 不过其实完全不同, 2pc的过程基于协调者, 他不是被选举出来的, 2pc是为了确定其他节点能不能成功执行, 并且要求所有RM都回复yes. 共识算法中, 只要获取多数节点投票

发生主节点变换后, 针对数据不一致的节点(纪元编号不对)还要进行数据恢复.

上述过程是一个基于共识协议的写入一致性保证过程

综上, 这套基于proposal的算法可以成功在"谁是主节点"这个值上达成共识, 有了主节点, 进行全序广播就很简单了.


## 共识算法局限性

共识算法能在不稳定的环境中保证安全性(一致, 正直, 有效, 容错)

并可以实现全序广播, 进而构建可以容错的线性一致系统

但是共识算法也有缺陷:
- 同步复制, 如果每次都要多数节点确认, 那这就成了同步复制, 性能太差, 大家都去异步复制追求性能了, 所以数据持久性是不能保证的, 如果一个主节点被替换掉, 部分从节点就会被要求进行数据恢复, 从而丢弃老主节点的一些数据
- 必须基于集群, 要容错一个节点就得三节点集群, 容错两个就得五个节点集群, 只有联通的多数派集群能正常工作
- 动态成员变更复杂, 由于投票依赖于成员个数, 所以增删节点也需要整个系统的配置
	- 有些算法有动态变更算法, 这非常复杂
- 网络环境, 网络很差, 共识算法也没招了, 共识算法要一直选主, 这时候还是想想降级吧

特别是网络环境, 共识算法对这个非常敏感, 因为节点根本判断不了主节点是宕机了还是网络波动, 为了性能又不能让超时阈值设置太高.
比如有时候整个网络都正常, 只有少数网络节点抖动, 此时Raft就会在局部进行疯狂选主, 或者不断尝试挑战联通多数派的主节点, 此时就会宕机

- 不过pre-vote似乎可以解决
- 低网情况的共识算法还需要进一步优化.

# 成员关系与协调服务

zk和etcd往往被描述成分布式KV存储, 或者分布式协调与配置服务, 他们的api看起来非常像一个数据库: 可以读写kv, 也可以遍历一组keys

他们的设计目标是, 存储小尺度的数据, 并在一个较小的集群上对这些数据保持共识
- 即在zk内部实现共识算法/全序广播, 进而为外部系统提供全序关系

在此基础上, 他们提供这些功能:
- 线性化原子操作 lock
	- 共识协议保证对lock值的并发操作只成功一个, 并在网络故障和zk部分宕机情况下保持原子和线性.
	- 一般来说, 实现成一个带过期时间的租约形式, 防止client宕机导致不释放
- 操作全序保证 zxid
	- 使用防护令牌机制防止进程停顿(stw)带来的加锁冲突, 他在每次获取锁的时候都会单调自增[[知识,真相与谎言#领导者与锁#锁 / 租约]]
	- zk中, 给每个写操作分配一个全局自增的事务zxid和一个版本号cversion实现
- 故障检测机制(ephemeral node)
	- client和zk维持一个长会话, 并用临时节点与会话绑定
	- 如果达到心跳的超时阈值, zk将这个节点清理, 并释放所有相关的锁
- 变动通知机制(watch)
	- 支持client去watch一些值的变化, 删除和增加等
	- client会在变化时收到通知, 不用去polling
## 节点分配任务

zk可以支持业务集群, 特别是有状态集群的选主和负载再均衡操作

通过在zk内部形成共识, 指定业务集群的主节点
## 服务发现

zk, etcd, consul都会被用于sd, 这个操作不是很依赖共识协议, 是kv存储的应用

放在zk等组件上, 主要是其存储量较小, 和对HA和网络稳定的需求

这种情况, 可以在集群中设置只读的副本, 如raft中的learner, 不参与共识协议过程, 只从集群中异步接收数据, 并用于向外部传达
## 成员服务

成员服务主要是为了判定哪些节点是存活的, 在无界网络延迟中, 无法可靠的检测一个节点是否故障

不过使用故障检测和共识算法, 所有节点的状态都会在集群中达成共识
- 是否真的故障不重要, 重要的是, 必须统一处理, 防止不一致

比如设计一个使用最小id进行选主的算法, 当前最小id, 即当前leader是否真的宕机不重要, 重要的是, 所有集群中节点要一起认为当前leader下线, 并选出一个新的最小id

